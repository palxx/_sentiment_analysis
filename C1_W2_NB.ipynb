{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/palxx/_sentiment_analysis/blob/main/C1_W2_NB.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vwHxHQCp4ZV7"
      },
      "source": [
        "Naive Bayes\n",
        "\n",
        "* Train a naive bayes model on a sentiment analysis task\n",
        "* Test using your model\n",
        "* Compute ratios of positive words to negative words\n",
        "* Do some error analysis\n",
        "* Predict on your own tweet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fzipaJPv4ZV_",
        "outputId": "77e2bebf-9427-490a-f352-d7d75a0c2a2f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package twitter_samples to /root/nltk_data...\n",
            "[nltk_data]   Package twitter_samples is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ],
      "source": [
        "import pdb\n",
        "from nltk.corpus import stopwords, twitter_samples\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import nltk\n",
        "import string\n",
        "from nltk.tokenize import TweetTokenizer\n",
        "from os import getcwd\n",
        "\n",
        "nltk.download('twitter_samples')\n",
        "nltk.download('stopwords')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import string\n",
        "\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import PorterStemmer\n",
        "from nltk.tokenize import TweetTokenizer\n",
        "\n",
        "from matplotlib.patches import Ellipse\n",
        "import matplotlib.transforms as transforms\n",
        "\n",
        "import numpy as np # Library for linear algebra and math utils\n",
        "\n",
        "\n",
        "def process_tweet(tweet):\n",
        "    '''\n",
        "    Input:\n",
        "        tweet: a string containing a tweet\n",
        "    Output:\n",
        "        tweets_clean: a list of words containing the processed tweet\n",
        "\n",
        "    '''\n",
        "    stemmer = PorterStemmer()\n",
        "    stopwords_english = stopwords.words('english')\n",
        "    # remove stock market tickers like $GE\n",
        "    tweet = re.sub(r'\\$\\w*', '', tweet)\n",
        "    # remove old style retweet text \"RT\"\n",
        "    tweet = re.sub(r'^RT[\\s]+', '', tweet)\n",
        "    # remove hyperlinks\n",
        "    #tweet = re.sub(r'https?:\\/\\/.*[\\r\\n]*', '', tweet)\n",
        "    tweet = re.sub(r'https?://[^\\s\\n\\r]+', '', tweet)\n",
        "    # remove hashtags\n",
        "    # only removing the hash # sign from the word\n",
        "    tweet = re.sub(r'#', '', tweet)\n",
        "    # tokenize tweets\n",
        "    tokenizer = TweetTokenizer(preserve_case=False, strip_handles=True,\n",
        "                               reduce_len=True)\n",
        "    tweet_tokens = tokenizer.tokenize(tweet)\n",
        "\n",
        "    tweets_clean = []\n",
        "    for word in tweet_tokens:\n",
        "        if (word not in stopwords_english and  # remove stopwords\n",
        "            word not in string.punctuation):  # remove punctuation\n",
        "            # tweets_clean.append(word)\n",
        "            stem_word = stemmer.stem(word)  # stemming word\n",
        "            tweets_clean.append(stem_word)\n",
        "\n",
        "    return tweets_clean\n",
        "\n",
        "\n",
        "def test_lookup(func):\n",
        "    freqs = {('sad', 0): 4,\n",
        "             ('happy', 1): 12,\n",
        "             ('oppressed', 0): 7}\n",
        "    word = 'happy'\n",
        "    label = 1\n",
        "    if func(freqs, word, label) == 12:\n",
        "        return 'SUCCESS!!'\n",
        "    return 'Failed Sanity Check!'\n",
        "\n",
        "\n",
        "def lookup(freqs, word, label):\n",
        "    '''\n",
        "    Input:\n",
        "        freqs: a dictionary with the frequency of each pair (or tuple)\n",
        "        word: the word to look up\n",
        "        label: the label corresponding to the word\n",
        "    Output:\n",
        "        n: the number of times the word with its corresponding label appears.\n",
        "    '''\n",
        "    n = 0  # freqs.get((word, label), 0)\n",
        "\n",
        "    pair = (word, label)\n",
        "    if (pair in freqs):\n",
        "        n = freqs[pair]\n",
        "\n",
        "    return n\n",
        "\n",
        "# From: https://matplotlib.org/3.1.1/gallery/statistics/confidence_ellipse.html#sphx-glr-gallery-statistics-confidence-ellipse-py\n",
        "\n",
        "\n",
        "def confidence_ellipse(x, y, ax, n_std=3.0, facecolor='none', **kwargs):\n",
        "    \"\"\"\n",
        "    Create a plot of the covariance confidence ellipse of `x` and `y`\n",
        "    Parameters\n",
        "    ----------\n",
        "    x, y : array_like, shape (n, )\n",
        "        Input data.\n",
        "    ax : matplotlib.axes.Axes\n",
        "        The axes object to draw the ellipse into.\n",
        "    n_std : float\n",
        "        The number of standard deviations to determine the ellipse's radiuses.\n",
        "    Returns\n",
        "    -------\n",
        "    matplotlib.patches.Ellipse\n",
        "    Other parameters\n",
        "    ----------------\n",
        "    kwargs : `~matplotlib.patches.Patch` properties\n",
        "    \"\"\"\n",
        "    if x.size != y.size:\n",
        "        raise ValueError(\"x and y must be the same size\")\n",
        "\n",
        "    cov = np.cov(x, y)\n",
        "    pearson = cov[0, 1] / np.sqrt(cov[0, 0] * cov[1, 1])\n",
        "    # Using a special case to obtain the eigenvalues of this\n",
        "    # two-dimensionl dataset.\n",
        "    ell_radius_x = np.sqrt(1 + pearson)\n",
        "    ell_radius_y = np.sqrt(1 - pearson)\n",
        "    ellipse = Ellipse((0, 0),\n",
        "                      width=ell_radius_x * 2,\n",
        "                      height=ell_radius_y * 2,\n",
        "                      facecolor=facecolor,\n",
        "                      **kwargs)\n",
        "\n",
        "    # Calculating the stdandard deviation of x from\n",
        "    # the squareroot of the variance and multiplying\n",
        "    # with the given number of standard deviations.\n",
        "    scale_x = np.sqrt(cov[0, 0]) * n_std\n",
        "    mean_x = np.mean(x)\n",
        "\n",
        "    # calculating the stdandard deviation of y ...\n",
        "    scale_y = np.sqrt(cov[1, 1]) * n_std\n",
        "    mean_y = np.mean(y)\n",
        "\n",
        "    transf = transforms.Affine2D() \\\n",
        "        .rotate_deg(45) \\\n",
        "        .scale(scale_x, scale_y) \\\n",
        "        .translate(mean_x, mean_y)\n",
        "\n",
        "    ellipse.set_transform(transf + ax.transData)\n",
        "    return ax.add_patch(ellipse)\n"
      ],
      "metadata": {
        "id": "VHEV9CSmil7c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ze56kkUD4ZWB"
      },
      "outputs": [],
      "source": [
        "filePath = f\"{getcwd()}/../tmp2/\"\n",
        "nltk.data.path.append(filePath)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vq56jSF14ZWB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "71bd9437-492f-4d29-af51-b9796e8ef855"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train_x: 8000 tweets, train_y: (8000,)\n",
            "test_x: 2000 tweets, test_y: (2000,)\n"
          ]
        }
      ],
      "source": [
        "from nltk.corpus import twitter_samples\n",
        "import numpy as np\n",
        "\n",
        "# Get the sets of positive and negative tweets\n",
        "all_positive_tweets = twitter_samples.strings('positive_tweets.json')\n",
        "all_negative_tweets = twitter_samples.strings('negative_tweets.json')\n",
        "\n",
        "# Split the data into two pieces, one for training and one for testing (validation set)\n",
        "train_pos, test_pos = all_positive_tweets[:4000], all_positive_tweets[4000:]\n",
        "train_neg, test_neg = all_negative_tweets[:4000], all_negative_tweets[4000:]\n",
        "\n",
        "# Combine positive and negative tweets\n",
        "train_x = train_pos + train_neg\n",
        "test_x = test_pos + test_neg\n",
        "\n",
        "# Create labels: 1 for positive, 0 for negative\n",
        "train_y = np.hstack((np.ones(len(train_pos)), np.zeros(len(train_neg))))\n",
        "test_y = np.hstack((np.ones(len(test_pos)), np.zeros(len(test_neg))))\n",
        "\n",
        "print(f\"train_x: {len(train_x)} tweets, train_y: {train_y.shape}\")\n",
        "print(f\"test_x: {len(test_x)} tweets, test_y: {test_y.shape}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3myq2Bi84ZWB"
      },
      "source": [
        "<a name='1'></a>\n",
        "## Process the Data\n",
        "\n",
        "- **Remove noise**: You will first want to remove noise from your data -- that is, remove words that don't tell you much about the content. These include all common words like 'I, you, are, is, etc...'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JM_ICt5F4ZWC",
        "outputId": "3f1ba4af-6928-4efe-fe60-ae083b2742e1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['hello', 'great', 'day', ':)', 'good', 'morn']\n"
          ]
        }
      ],
      "source": [
        "custom_tweet = \"RT @Twitter @chapagain Hello There! Have a great day. :) #good #morning http://chapagain.com.np\"\n",
        "\n",
        "# print cleaned tweet\n",
        "print(process_tweet(custom_tweet))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qmheA-Kc4ZWD"
      },
      "outputs": [],
      "source": [
        "def count_tweets(result, tweets, ys):\n",
        "    '''\n",
        "    Input:\n",
        "        result: a dictionary that will be used to map each pair to its frequency\n",
        "        tweets: a list of tweets\n",
        "        ys: a list corresponding to the sentiment of each tweet (either 0 or 1)\n",
        "    Output:\n",
        "        result: a dictionary mapping each pair to its frequency\n",
        "    '''\n",
        "    for y, tweet in zip(ys, tweets):\n",
        "        for word in process_tweet(tweet):\n",
        "            # define the key, which is the word and label tuple\n",
        "            #print(word)\n",
        "            pair = (word, y)\n",
        "            #print(pair)\n",
        "\n",
        "            # if the key exists in the dictionary, increment the count\n",
        "            if pair in result:\n",
        "                result[pair] += 1\n",
        "\n",
        "            # else, if the key is new, add it to the dictionary and set the count to 1\n",
        "            else:\n",
        "                result[pair] = 1\n",
        "\n",
        "    return result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n2c3D-f84ZWD",
        "outputId": "5ef8201a-5df9-4637-8195-5452fa7ea773"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{('happi', 1): 1, ('trick', 0): 1, ('sad', 0): 1, ('tire', 0): 2}"
            ]
          },
          "metadata": {},
          "execution_count": 39
        }
      ],
      "source": [
        "# Testing your function\n",
        "result = {}\n",
        "tweets = ['i am happy', 'i am tricked', 'i am sad', 'i am tired', 'i am tired']\n",
        "ys = [1, 0, 0, 0, 0]\n",
        "count_tweets(result, tweets, ys)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zyQbZIk24ZWE"
      },
      "outputs": [],
      "source": [
        "# Build the freqs dictionary for later uses\n",
        "freqs = count_tweets({}, train_x, train_y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WmSHj8fl4ZWE"
      },
      "outputs": [],
      "source": [
        "def train_naive_bayes(freqs, train_x, train_y):\n",
        "    '''\n",
        "    Input:\n",
        "        freqs: dictionary from (word, label) to how often the word appears\n",
        "        train_x: a list of tweets\n",
        "        train_y: a list of labels corresponding to the tweets (0,1)\n",
        "    Output:\n",
        "        logprior: the log prior. (equation 3 above)\n",
        "        loglikelihood: the log likelihood of you Naive bayes equation. (equation 6 above)\n",
        "    '''\n",
        "    loglikelihood = {}\n",
        "    logprior = 0\n",
        "    ### START CODE HERE ###\n",
        "\n",
        "    # calculate V, the number of unique words in the vocabulary\n",
        "    vocab = set([pair[0] for pair in freqs.keys()])\n",
        "    #len(vocab)\n",
        "    V = len(vocab)\n",
        "    #print(V)\n",
        "\n",
        "    # calculate N_pos, N_neg, V_pos, V_neg\n",
        "    N_pos = N_neg = 0\n",
        "    for pair in freqs.keys():\n",
        "        # if the label is positive (greater than zero)\n",
        "        if pair[1] > 0:\n",
        "\n",
        "            # Increment the number of positive words by the count for this (word, label) pair\n",
        "            N_pos += freqs[pair]\n",
        "\n",
        "        # else, the label is negative\n",
        "        else:\n",
        "\n",
        "            # increment the number of negative words by the count for this (word,label) pair\n",
        "            N_neg += freqs[pair]\n",
        "\n",
        "\n",
        "    # Calculate D, the number of documents\n",
        "    D = len(train_y)\n",
        "\n",
        "\n",
        "    # Calculate D_pos, the number of positive documents\n",
        "    D_pos = sum(train_y)\n",
        "\n",
        "\n",
        "    # Calculate D_neg, the number of negative documents\n",
        "    D_neg = D - D_pos\n",
        "\n",
        "    # Calculate logprior\n",
        "    logprior = np.log(D_pos) - np.log(D_neg)\n",
        "\n",
        "    # For each word in the vocabulary...\n",
        "    for word in vocab:\n",
        "        # get the positive and negative frequency of the word\n",
        "        #print(word)\n",
        "        freq_pos = freqs.get((word, 1), 0)\n",
        "        freq_neg = freqs.get((word, 0), 0)\n",
        "\n",
        "        # calculate the probability that each word is positive, and negative\n",
        "        p_w_pos = (freq_pos + 1)/(N_pos + V)\n",
        "        p_w_neg = (freq_neg + 1)/(N_neg + V)\n",
        "\n",
        "        # calculate the log likelihood of the word\n",
        "        loglikelihood[word] = np.log(p_w_pos/p_w_neg)\n",
        "\n",
        "    ### END CODE HERE ###\n",
        "\n",
        "    return logprior, loglikelihood"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "deletable": false,
        "editable": false,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QSNYVV9w4ZWE",
        "outputId": "0cdcea2b-ea89-4325-9bfd-fd07b0413c91"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.0\n",
            "9161\n"
          ]
        }
      ],
      "source": [
        "logprior, loglikelihood = train_naive_bayes(freqs, train_x, train_y)\n",
        "print(logprior)\n",
        "print(len(loglikelihood))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k2yC48hz4ZWF"
      },
      "outputs": [],
      "source": [
        "def naive_bayes_predict(tweet, logprior, loglikelihood):\n",
        "    '''\n",
        "    Input:\n",
        "        tweet: a string\n",
        "        logprior: a number\n",
        "        loglikelihood: a dictionary of words mapping to numbers\n",
        "    Output:\n",
        "        p: the sum of all the logliklihoods of each word in the tweet (if found in the dictionary) + logprior (a number)\n",
        "\n",
        "    '''\n",
        "    #print(loglikelihood)\n",
        "\n",
        "    # process the tweet to get a list of words\n",
        "    word_l = process_tweet(tweet)\n",
        "\n",
        "    # initialize probability to zero\n",
        "    p = 0\n",
        "\n",
        "    # add the logprior\n",
        "    p += logprior\n",
        "\n",
        "    for word in word_l:\n",
        "\n",
        "        # check if the word exists in the loglikelihood dictionary\n",
        "        if word in loglikelihood:\n",
        "            # add the log likelihood of that word to the probability\n",
        "            p += loglikelihood.get(word)\n",
        "\n",
        "\n",
        "\n",
        "    return p"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "deletable": false,
        "editable": false,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WjJDY6e74ZWF",
        "outputId": "3748d900-b2cc-4219-a53a-9a9fa27834eb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The expected output is -2.8344841452259555\n"
          ]
        }
      ],
      "source": [
        "my_tweet = 'She is sad.'\n",
        "p = naive_bayes_predict(my_tweet, logprior, loglikelihood)\n",
        "print('The expected output is', p)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A5tqKzaz4ZWF"
      },
      "outputs": [],
      "source": [
        "def test_naive_bayes(test_x, test_y, logprior, loglikelihood, naive_bayes_predict=naive_bayes_predict):\n",
        "    \"\"\"\n",
        "    Input:\n",
        "        test_x: A list of tweets\n",
        "        test_y: the corresponding labels for the list of tweets\n",
        "        logprior: the logprior\n",
        "        loglikelihood: a dictionary with the loglikelihoods for each word\n",
        "    Output:\n",
        "        accuracy: (# of tweets classified correctly)/(total # of tweets)\n",
        "    \"\"\"\n",
        "    accuracy = 0  # return this properly\n",
        "\n",
        "    y_hats = []\n",
        "    for tweet in test_x:\n",
        "        # if the prediction is > 0\n",
        "        if naive_bayes_predict(tweet, logprior, loglikelihood) > 0:\n",
        "            # the predicted class is 1\n",
        "            y_hat_i = 1\n",
        "        else:\n",
        "            # otherwise the predicted class is 0\n",
        "            y_hat_i = 0\n",
        "\n",
        "        # append the predicted class to the list y_hats\n",
        "        y_hats.append(y_hat_i)\n",
        "\n",
        "    # error is the average of the absolute values of the differences between y_hats and test_y\n",
        "    error = np.mean(np.abs(y_hats - test_y))\n",
        "\n",
        "    # Accuracy is 1 minus the error\n",
        "    accuracy = 1 - error\n",
        "\n",
        "    return accuracy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7dLYRxVc4ZWF",
        "outputId": "cee7d425-113c-440a-9448-d274838dbf7d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Naive Bayes accuracy = 0.9955\n"
          ]
        }
      ],
      "source": [
        "print(\"Naive Bayes accuracy = %0.4f\" %\n",
        "      (test_naive_bayes(test_x, test_y, logprior, loglikelihood)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "deletable": false,
        "editable": false,
        "scrolled": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6fOcBNhB4ZWG",
        "outputId": "680faeea-c1f9-4cfa-bb76-57f9e969ac4a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "I am happy -> 2.14\n",
            "I am bad -> -1.31\n",
            "this movie should have been great. -> 2.12\n",
            "great -> 2.13\n",
            "great great -> 4.26\n",
            "great great great -> 6.39\n",
            "great great great great -> 8.52\n"
          ]
        }
      ],
      "source": [
        "# Test your function\n",
        "for tweet in ['I am happy', 'I am bad', 'this movie should have been great.', 'great', 'great great', 'great great great', 'great great great great']:\n",
        "    p = naive_bayes_predict(tweet, logprior, loglikelihood)\n",
        "    print(f'{tweet} -> {p:.2f}')"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}